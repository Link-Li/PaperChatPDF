<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:524.0pt;left:10.9pt;line-height:20.0pt"><span style="font-family:Times New Roman,serif;font-size:20.0pt;color:#919191">arXiv:1911.03977v3  [cs.AI]  10 Apr 2020</span></p>
<p style="top:25.6pt;left:49.0pt;line-height:7.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.0pt">TO APPEAR IN IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</span></p>
<p style="top:25.6pt;left:559.6pt;line-height:7.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.0pt">1</span></p>
<p style="top:57.7pt;left:59.4pt;line-height:23.9pt"><span style="font-family:NimbusRomNo9L,serif;font-size:23.9pt">Multimodal Intelligence: Representation Learning,</span></p>
<p style="top:85.5pt;left:122.4pt;line-height:23.9pt"><span style="font-family:NimbusRomNo9L,serif;font-size:23.9pt">Information Fusion, and Applications</span></p>
<p style="top:119.2pt;left:114.0pt;line-height:11.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:11.0pt">Chao Zhang, Zichao Yang, Xiaodong He,</span><span style="font-family:NimbusRomNo9L,serif;font-size:11.0pt"> Fellow, IEEE,</span><span style="font-family:NimbusRomNo9L,serif;font-size:11.0pt"> and Li Deng,</span><span style="font-family:NimbusRomNo9L,serif;font-size:11.0pt"> Fellow, IEEE</span></p>
<p style="top:176.8pt;left:58.9pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">Abstract</span><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">&#x2014;Deep learning methods have revolutionized speech</span></p>
<p style="top:186.7pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">recognition, image recognition, and natural language processing</span></p>
<p style="top:196.7pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">since 2010. Each of these tasks involves a single modality in</span></p>
<p style="top:206.7pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">their input signals. However, many applications in the arti&#xfb01;cial</span></p>
<p style="top:216.6pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">intelligence &#xfb01;eld involve multiple modalities. Therefore, it is of</span></p>
<p style="top:226.6pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">broad interest to study the more dif&#xfb01;cult and complex problem of</span></p>
<p style="top:236.5pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">modeling and learning across multiple modalities. In this paper,</span></p>
<p style="top:246.5pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">we provide a technical review of available models and learning</span></p>
<p style="top:256.5pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">methods for multimodal intelligence. The main focus of this re-</span></p>
<p style="top:266.4pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">view is the combination of vision and natural language modalities,</span></p>
<p style="top:276.4pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">which has become an important topic in both the computer vision</span></p>
<p style="top:286.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">and natural language processing research communities.</span></p>
<p style="top:296.5pt;left:58.9pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">This review provides a comprehensive analysis of recent works</span></p>
<p style="top:306.5pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">on multimodal deep learning from three perspectives: learning</span></p>
<p style="top:316.5pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">multimodal representations, fusing multimodal signals at various</span></p>
<p style="top:326.4pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">levels, and multimodal applications. Regarding multimodal rep-</span></p>
<p style="top:336.4pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">resentation learning, we review the key concepts of embedding,</span></p>
<p style="top:346.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">which unify multimodal signals into a single vector space and</span></p>
<p style="top:356.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">thereby enable cross-modality signal processing. We also review</span></p>
<p style="top:366.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">the properties of many types of embeddings that are constructed</span></p>
<p style="top:376.2pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">and learned for general downstream tasks. Regarding multimodal</span></p>
<p style="top:386.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">fusion, this review focuses on special architectures for the</span></p>
<p style="top:396.3pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">integration of representations of unimodal signals for a particular</span></p>
<p style="top:406.2pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">task. Regarding applications, selected areas of a broad interest</span></p>
<p style="top:416.2pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">in the current literature are covered, including image-to-text</span></p>
<p style="top:426.1pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">caption generation, text-to-image generation, and visual question</span></p>
<p style="top:436.1pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">answering. We believe that this review will facilitate future</span></p>
<p style="top:446.1pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">studies in the emerging &#xfb01;eld of multimodal intelligence for related</span></p>
<p style="top:456.0pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">communities.</span></p>
<p style="top:471.7pt;left:58.9pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">Index Terms</span><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">&#x2014;Multimodality, representation, multimodal fu-</span></p>
<p style="top:481.7pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">sion, deep learning, embedding, speech, vision, natural language,</span></p>
<p style="top:491.7pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">caption generation, text-to-image generation, visual question</span></p>
<p style="top:501.6pt;left:49.0pt;line-height:9.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:9.0pt">answering, visual reasoning</span></p>
<p style="top:530.8pt;left:135.6pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">I. I</span><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">NTRODUCTION</span></p>
<p style="top:545.3pt;left:49.0pt;line-height:28.7pt"><span style="font-family:NimbusRomNo9L,serif;font-size:28.7pt">S</span></p>
<p style="top:547.2pt;left:66.5pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">IGNIFICANT progress has been made in the &#xfb01;eld of</span></p>
<p style="top:559.2pt;left:66.5pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">machine learning in recent years based on the rapid</span></p>
<p style="top:571.2pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">development of deep learning algorithms [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">1</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">6</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. The &#xfb01;rst</span></p>
<p style="top:583.0pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">major milestone was a signi&#xfb01;cant increase in the accuracy of</span></p>
<p style="top:595.0pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">large-scale automatic speech recognition based on the use of</span></p>
<p style="top:607.0pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">fully connected deep neural networks (DNNs) and deep auto-</span></p>
<p style="top:618.9pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">encoders around 2010 [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">7</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">17</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. Shortly thereafter, a series of</span></p>
<p style="top:630.9pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">breakthroughs was achieved in computer vision (CV) using</span></p>
<p style="top:642.8pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">deep convolutional neural network (CNN) models [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">18</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">] for</span></p>
<p style="top:654.8pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">large-scale image classi&#xfb01;cation around 2012 [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">19</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">22</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">] and</span></p>
<p style="top:666.8pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">large-scale object detection around 2014 [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">23</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">25</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. All of</span></p>
<p style="top:678.7pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">these milestones have been achieved for pattern recognition</span></p>
<p style="top:690.7pt;left:49.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">with a single input modality. In natural language processing</span></p>
<p style="top:712.7pt;left:56.9pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">C. Zhang and X. He were with JD AI Research. C. Zhang was also with</span></p>
<p style="top:721.5pt;left:49.0pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">the Department of Engineering, University of Cambridge, UK.</span></p>
<p style="top:730.5pt;left:56.9pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">Z. Yang and L. Deng were with Citadel LLC.</span></p>
<p style="top:739.5pt;left:56.9pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">Corresponding author: Xiaodong He</span><span style="font-family:CMMI8,serif;font-size:8.0pt"> &lt;</span><span style="font-family:NimbusMonL,serif;font-size:8.0pt">xiaodong.he@jd.com</span><span style="font-family:CMMI8,serif;font-size:8.0pt">&gt;</span></p>
<p style="top:176.0pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">(NLP), recurrent neural network (RNN) based semantic slot</span></p>
<p style="top:188.0pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">&#xfb01;lling methods [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">26</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">] have achieved state-of-the-art for spoken</span></p>
<p style="top:200.0pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">language understanding. RNN-encoder-decoder models with</span></p>
<p style="top:211.9pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">attention mechanisms [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">27</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], which are also referred to as</span></p>
<p style="top:223.9pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">sequence-to-sequence models [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">28</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], have achieved superior</span></p>
<p style="top:235.8pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">performance for machine translation in an end-to-end fashion</span></p>
<p style="top:247.8pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">29</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">30</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. For additional NLP tasks with small amounts of</span></p>
<p style="top:259.7pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">training data, such as question answering (QA) and machine</span></p>
<p style="top:271.6pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">reading comprehension, generative pre-training has achieved</span></p>
<p style="top:283.6pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">state-of-the-art results [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">31</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">33</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. This method transfers param-</span></p>
<p style="top:295.6pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">eters from a language model (LM) pre-trained on a large out-</span></p>
<p style="top:307.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">of-domain dataset using unsupervised training or self-training,</span></p>
<p style="top:319.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">which is followed by &#xfb01;ne-tuning on small in-domain datasets.</span></p>
<p style="top:331.0pt;left:322.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">Although there have been signi&#xfb01;cant advances in vision,</span></p>
<p style="top:343.0pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">speech, and language processing, many problems in the ar-</span></p>
<p style="top:354.9pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">ti&#xfb01;cial intelligence &#xfb01;eld involve more than one input modal-</span></p>
<p style="top:366.9pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">ity, such as intelligent personal assistant systems that must</span></p>
<p style="top:378.9pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">understand human communication based on spoken words,</span></p>
<p style="top:390.8pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">body language, and pictorial languages [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">34</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. Therefore, it is</span></p>
<p style="top:402.8pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">of broad interest to study modeling and training approaches</span></p>
<p style="top:414.8pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">across multiple modalities [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">35</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. Based on advances in image</span></p>
<p style="top:426.7pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">processing and language understanding [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">36</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], tasks combining</span></p>
<p style="top:438.7pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">images and text have attracted signi&#xfb01;cant attention, including</span></p>
<p style="top:450.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">visual-based referred expression understanding and phrase</span></p>
<p style="top:462.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">localization [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">37</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">39</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], as well as image and video captioning</span></p>
<p style="top:474.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">40</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">45</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], visual QA (VQA) [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">46</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">48</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], text-to-image gen-</span></p>
<p style="top:486.4pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">eration [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">49</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">51</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], and visual-and-language navigation [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">52</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">].</span></p>
<p style="top:498.4pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">In these tasks, natural language plays a key role in helping</span></p>
<p style="top:510.4pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">machines in &#x201c;understanding&#x201d; the content of images, where</span></p>
<p style="top:522.3pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">&#x201c;understanding&#x201d; means capturing the underlying correlations</span></p>
<p style="top:534.3pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">between the semantics embedded in languages and the visual</span></p>
<p style="top:546.2pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">features obtained from images. In addition to text, vision can</span></p>
<p style="top:558.2pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">also be combined with speech to perform audio-visual speech</span></p>
<p style="top:570.2pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">recognition [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">53</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">55</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], speaker recognition [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">56</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]&#x2013;[</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">58</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], speaker</span></p>
<p style="top:582.1pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">diarization, [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">59</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">60</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], as well as speech separation [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">61</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">62</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]</span></p>
<p style="top:594.1pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">and enhancement [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#00ff00">63</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">].</span></p>
<p style="top:605.6pt;left:322.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">This paper provides a technical review of the models and</span></p>
<p style="top:617.6pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">training methods used for multimodal intelligence. Our main</span></p>
<p style="top:629.6pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">focus is the combination of CV and NLP, which has become</span></p>
<p style="top:641.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">an important area for both of these research communities that</span></p>
<p style="top:653.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">covers many different tasks and technologies. To provide a</span></p>
<p style="top:665.5pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">structured perspective, we have organized this technical review</span></p>
<p style="top:677.3pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">according to three key topics: representations, fusion, and</span></p>
<p style="top:689.3pt;left:312.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">applications.</span></p>
<p style="top:704.5pt;left:322.0pt;line-height:7.0pt"><span style="font-family:CMSY7,serif;font-size:7.0pt">&#x2022;</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt"> Learning representations for input data is a core problem</span></p>
<p style="top:714.0pt;left:332.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">in deep learning. For multimodal tasks, collecting parallel</span></p>
<p style="top:726.0pt;left:332.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">data across different modalities can be a dif&#xfb01;cult task.</span></p>
<p style="top:738.0pt;left:332.0pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">Leveraging pre-trained representations with the desired</span></p>
</div>
