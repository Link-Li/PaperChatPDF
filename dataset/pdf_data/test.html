<div id="page0" style="width:612.0pt;height:792.0pt">
<p style="top:105.5pt;left:148.1pt;line-height:14.3pt"><b><span style="font-family:NimbusRomNo9L,serif;font-size:14.3pt">VL-GPT: A Generative Pre-trained Transformer</span></b></p>
<p style="top:123.4pt;left:124.7pt;line-height:14.3pt"><b><span style="font-family:NimbusRomNo9L,serif;font-size:14.3pt">for Vision and Language Understanding and Generation</span></b></p>
<p style="top:163.2pt;left:149.4pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Jinguo Zhu</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">1*</span></sup></p>
<p style="top:163.2pt;left:224.6pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Xiaohan Ding</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">2*</span></sup></p>
<p style="top:163.2pt;left:317.0pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Yixiao Ge</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">2,3</span></sup></p>
<p style="top:163.2pt;left:389.9pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Yuying Ge</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">2</span></sup></p>
<p style="top:177.1pt;left:131.2pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Sijie Zhao</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">2</span></sup></p>
<p style="top:177.1pt;left:198.8pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Hengshuang Zhao</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">4</span></sup></p>
<p style="top:177.1pt;left:307.2pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Xiaohua Wang</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">1</span></sup></p>
<p style="top:177.1pt;left:396.4pt;line-height:12.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Ying Shan</span><sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">2,3</span></sup><sup><span style="font-family:MarVoSym,serif;font-size:8.0pt">&#xfffd;</span></sup></p>
<p style="top:191.2pt;left:196.0pt;line-height:7.3pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.3pt">1</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.4pt"> Xi&#x2019;an Jiaotong University</span></p>
<p style="top:191.2pt;left:330.7pt;line-height:7.3pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.3pt">2</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.4pt"> Tencent AI Lab</span></p>
<p style="top:205.2pt;left:178.8pt;line-height:7.3pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.3pt">3</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.4pt"> ARC Lab, Tencent PCG</span></p>
<p style="top:205.2pt;left:306.8pt;line-height:7.3pt"><span style="font-family:NimbusRomNo9L,serif;font-size:7.3pt">4</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.4pt"> The University of Hong Kong</span></p>
<p style="top:253.1pt;left:146.0pt;line-height:12.0pt"><b><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">Abstract</span></b></p>
<p style="top:278.7pt;left:62.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">In this work, we introduce Vision-Language Generative</span></i></p>
<p style="top:290.7pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">Pre-trained Transformer (VL-GPT), a transformer model</span></i></p>
<p style="top:302.6pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">proficient at concurrently perceiving and generating visual</span></i></p>
<p style="top:314.6pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">and linguistic data. VL-GPT achieves a unified pre-training</span></i></p>
<p style="top:326.5pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">approach for both image and text modalities by employing a</span></i></p>
<p style="top:338.5pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">straightforward auto-regressive objective, thereby enabling</span></i></p>
<p style="top:350.5pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">the model to process image and text as seamlessly as a lan-</span></i></p>
<p style="top:362.4pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">guage model processes text. To accomplish this, we initially</span></i></p>
<p style="top:374.4pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">propose a novel image tokenizer-detokenizer framework for</span></i></p>
<p style="top:386.3pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">visual data, specifically designed to transform raw images</span></i></p>
<p style="top:398.3pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">into a sequence of continuous embeddings and reconstruct</span></i></p>
<p style="top:410.2pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">them accordingly. In combination with the existing text tok-</span></i></p>
<p style="top:422.2pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">enizer and detokenizer, this framework allows for the en-</span></i></p>
<p style="top:434.1pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">coding of interleaved image-text data into a multimodal</span></i></p>
<p style="top:446.1pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">sequence, which can subsequently be fed into the trans-</span></i></p>
<p style="top:458.1pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">former model. Consequently, VL-GPT can perform large-</span></i></p>
<p style="top:470.0pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">scale pre-training on multimodal corpora utilizing a uni-</span></i></p>
<p style="top:482.0pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">fied auto-regressive objective (</span></i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">i.e</span><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">., next-token prediction).</span></i></p>
<p style="top:493.9pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">Upon completion of pre-training, VL-GPT exhibits remark-</span></i></p>
<p style="top:505.9pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">able zero-shot and few-shot performance across a diverse</span></i></p>
<p style="top:517.8pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">range of vision and language understanding and genera-</span></i></p>
<p style="top:529.8pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">tion tasks, including image captioning, visual question an-</span></i></p>
<p style="top:541.7pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">swering, text-to-image generation, and more. Additionally,</span></i></p>
<p style="top:553.7pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">the pre-trained model retrains in-context learning capabil-</span></i></p>
<p style="top:565.6pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">ities when provided with multimodal prompts. We further</span></i></p>
<p style="top:577.6pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">conduct instruction tuning on our VL-GPT, highlighting its</span></i></p>
<p style="top:589.6pt;left:50.1pt;line-height:10.0pt"><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">exceptional potential for multimodal assistance.</span></i></p>
<p style="top:624.0pt;left:50.1pt;line-height:12.0pt"><b><span style="font-family:NimbusRomNo9L,serif;font-size:12.0pt">1. Introduction</span></b></p>
<p style="top:644.6pt;left:50.1pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">Driven by the remarkable success of large language mod-</span></p>
<p style="top:656.6pt;left:50.1pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">els (LLMs) in the field of natural language processing</span></p>
<p style="top:676.3pt;left:61.0pt;line-height:6.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:6.0pt">*</span><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">Equal contribution. This work is done when Jinguo Zhu is an intern</span></p>
<p style="top:685.7pt;left:50.1pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">at Tencent AI Lab. The source code and model weights shall be released</span></p>
<p style="top:695.2pt;left:50.1pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">at</span><span style="font-family:NimbusMonL,serif;font-size:8.0pt;color:#ec008b"> https://github.com/AILab-CVC/VL-GPT</span><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">.</span><span style="font-family:MarVoSym,serif;font-size:6.0pt"> </span><sup><span style="font-family:MarVoSym,serif;font-size:6.0pt">&#xfffd;</span></sup><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">Corresponding</span></p>
<p style="top:704.6pt;left:50.1pt;line-height:8.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:8.0pt">author.</span></p>
<p style="top:254.7pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">(NLP) [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">40</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 41</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 54</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], there has been a surge of interest within</span></p>
<p style="top:266.7pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">multimodal community to develop large vision-language</span></p>
<p style="top:278.6pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">(VL) models. One of the promising approaches, exempli-</span></p>
<p style="top:290.6pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">fied by Flamingo [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">1</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], BLIP2 [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">24</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], LLAVA [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">25</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">], have ex-</span></p>
<p style="top:302.6pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">plored how to build large VL models based on powerful</span></p>
<p style="top:314.5pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">pre-trained LLMs. These studies typically adopted a similar</span></p>
<p style="top:326.5pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">architecture: a pre-trained image encoder and an LLM are</span></p>
<p style="top:338.4pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">connected via a trainable connection module, which aligns</span></p>
<p style="top:350.4pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">the image feature and text embeddings, thereby enabling</span></p>
<p style="top:362.3pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">language models to accept images and text as inputs and</span></p>
<p style="top:374.3pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">generate a text sequence.</span></p>
<p style="top:388.2pt;left:320.8pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">To expand the capabilities of generating image in a mul-</span></p>
<p style="top:400.2pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">timodal context, certain efforts,</span><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt"> e.g</span></i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">., Visual ChatGPT [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">47</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">],</span></p>
<p style="top:412.1pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">attempt to connect LLMs with image generation tools in a</span></p>
<p style="top:424.1pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">cascaded pipeline by transferring text messages, which in-</span></p>
<p style="top:436.0pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">evitably introduce instability and noise. Alternatively, an-</span></p>
<p style="top:448.0pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">other line of research achieves it by optimizing models in</span></p>
<p style="top:460.0pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">an end-to-end manner [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">9</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 18</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 23</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 30</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 48</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">]. By aligning the</span></p>
<p style="top:471.9pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">output space with the image diffusion models, VL models</span></p>
<p style="top:483.9pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">can not only perceive but also generate images and text.</span></p>
<p style="top:497.8pt;left:320.8pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">A crucial characteristic of large language models is auto-</span></p>
<p style="top:509.8pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">regressive modeling [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">31</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">],</span><i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt"> i.e</span></i><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">., predicting next token, which</span></p>
<p style="top:521.7pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">facilitates language understanding and generation in a uni-</span></p>
<p style="top:533.7pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">fied manner. However, in the aforementioned studies, the</span></p>
<p style="top:545.6pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">inconsistency of image embeddings between LLM&#x2019;s input</span></p>
<p style="top:557.6pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">and output sides compels the model to treat input images</span></p>
<p style="top:569.5pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">and generated images differently, resulting in separate mod-</span></p>
<p style="top:581.5pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">eling for image understanding and generation. Meanwhile,</span></p>
<p style="top:593.5pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">this discrepancy also obstructs the implementation of auto-</span></p>
<p style="top:605.4pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">regressive training loss on image embeddings.</span></p>
<p style="top:619.3pt;left:320.8pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">In this study, we introduce VL-GPT, a large vision-</span></p>
<p style="top:631.3pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">language generative pre-trained transformer that enables the</span></p>
<p style="top:643.3pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">unified training of both visual and linguistic data using an</span></p>
<p style="top:655.2pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">auto-regressive objective, as depicted in Fig.</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#ff0000"> 1</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">. To achieve</span></p>
<p style="top:667.2pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">this, we propose an image tokenizer-detokenizer framework</span></p>
<p style="top:679.1pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">for the conversion between raw image pixels and contin-</span></p>
<p style="top:691.1pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">uous visual embeddings, analogous to the role of the text</span></p>
<p style="top:703.0pt;left:308.9pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">tokenization [</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc">19</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">,</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt;color:#357cbc"> 43</span><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">] in language models. The framework</span></p>
<p style="top:732.9pt;left:295.1pt;line-height:10.0pt"><span style="font-family:NimbusRomNo9L,serif;font-size:10.0pt">1</span></p>
<p style="top:544.0pt;left:10.9pt;line-height:20.0pt"><span style="font-family:Times New Roman,serif;font-size:20.0pt;color:#919191">arXiv:2312.09251v1  [cs.CV]  14 Dec 2023</span></p>
</div>
